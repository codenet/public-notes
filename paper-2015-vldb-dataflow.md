# The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing

Ok, so basically [[paper-2013-vldb-millwheel]] from Google presented the idea of
"low watermarks". Low watermarks advance as data flows through the system. It is
a guarantee of each node in the system that no data earlier than the low
watermark of the node will be generated by the node. Low watermark is
bootstrapped by the input. Low watermark will be incremented by the input node
allowing some lag from the current time; input node will **drop** records
earlier than the low watermark to keep up with its promise of not producing
records earlier than its low watermark.

> Measurement of pending work in external systems is often an estimate, so in 
> practice, computations should expect a small rate of late records – records
> behind the low watermark – from such systems. Zeitgeist deals with this by
> dropping such data, while keeping track of how much data was dropped
> (empirically around 0.001% of records). Other pipelines retroactively correct
> their aggregates if late records arrive. Though this is not reflected in the
> above definition, the system guarantees that a computation’s low watermark is
> monotonic even in the face of late data. By waiting for the low watermark of a
> computation to advance past a certain value, the user can determine that they
> have a complete picture of their data up to that time, as previously
> illustrated by Zeitgeist’s dip detection. When assigning timestamps to new or 
> aggregate records, it is up to the user to pick a timestamp no smaller than
> any of the source records.

The paper does mention dealing with late data, but it seems to be done in an 
ad-hoc manner.

> While some computations (like spike detection in Zeitgeist) do not need 
> timers, many computations (like dip detection) use timers to wait for the low 
> watermark to advance before outputting aggregates. For these computations, 
> the low watermark’s lag behind real time bounds the freshness of these 
> aggregates. Since the low watermark propagates from injectors through the 
> computation graph, we expect the lag of a computation’s low watermark to be 
> proportional to its maximum pipeline distance from an injector.

Indeed, some operators like aggregators are _blocking_ until the low watermark
rises.

**Spark streaming** [SOSP'13] just says that we will break down streaming
computation into mini batch. Their focus is just to retrofit Spark onto
streaming computation. They also mention same two approaches as millwheel:
either drop late records or the application fixes it somehow.

> Developers may wish to group records based on an external timestamp of when 
> an event happened, e.g., when a user clicked a link, and records may arrive 
> out of order. D-Streams provide two means to handle this case: 
> 
> 1. The system can wait for a limited “slack time” before starting to process 
>    each batch. 
> 2. User programs can correct for late records at the application level. 

**Trill** paper from Microsoft [VLDB'14] focuses on latency-throughput 
trade-offs in streaming computation. Basically the idea is that we can get a 
better throughput if we can batch read input data for a node and process the 
batch in one shot. They are very particular about data layout. They keep the 
input data of each node in a columnar format; batches can give good spatial 
locality of access and generates opportunities to use vector instructions since 
the same operation is being applied in a node to adjacent data. But, large 
batches hurt latency. Their proposal is to add _punctuations_ which are 
basically timers to break the batches. Punctuations seem similar to windowing, 
but they seem to have to do more with physical realization of operators 
(breaking the batch read) than to do with the logical notion of windows.

Ok, so then the the dataflow model by Google comes out with what they have
learned over the years with Millwheel and FlumeJava [PLDI'10] (a wrapper system
which runs on top of MapReduce). They amplify the importance of dealing with
late data. Late data is a fact of life in a streaming application which cannot
be ignored:

Low latency and correctness is required:
> Advertisers/content providers want to know how often and for how long their 
> videos are being watched, with which content/ads, and by which demographic 
> groups. They also want to know how much they are being charged/paid. They want
> all of this information as quickly as possible, so that they can adjust 
> budgets and bids, change targeting, tweak campaigns, and plan future 
> directions in as close to real time as possible. Since money is involved, 
> correctness is paramount.

Dealing with late data is necessary (a billing application can not just ignore
user sessions and not charge the advertiser for them):
> Recommended practice at the time was to use the watermark as a completion 
> metric, with ad hoc logic to deal with late data or changes in source data. 
> Lacking a principled system for updates and retractions, a team that processed 
> resource utilization statistics ended up leaving our platform to build a 
> custom solution.

They criticize watermarks in terms of both correctness and low-latency w.r.t.
late data:

> However, watermarks themselves have two major shortcomings with respect to 
> correctness: 
> * They are sometimes too fast, meaning there may be late data that arrives 
>   behind the watermark. For many distributed data sources, it is intractable 
>   to derive a completely perfect event time watermark, and thus impossible to 
>   rely on it solely if we want 100% correctness in our output data. 
> * They are sometimes too slow. Because they are a global progress metric, 
>   the watermark can be held back for the entire pipeline by a single slow 
>   datum. And even for healthy pipelines with little variability in event-time 
>   skew, the baseline level of skew may still be multiple minutes or more, 
>   depending upon the input source. As a result, using watermarks as the sole 
>   signal for emitting window results is likely to yield higher latency of 
>   overall results than, for example, a comparable Lambda Architecture 
>   pipeline. 
> 
> For these reasons, we postulate that watermarks alone are insufficient. 

So the idea is that watermarks should just be used as a rough
estimation of progress. Since, watermarks can _never_ be determined accurately,
because of arbitrarily delayed data, they should not be trusted. So, now how
should a groupby+aggregator output anything? Since, it has to wait for all the
data to have arrived before outputting the aggregate. And the stream is
infinite.

The paper talks about two ideas: windowing and triggering. Firstly, before doing
an aggregation it is mandatory to assign each record to one or more windows (a
record may be in multiple windows since the windows may overlap). Now, the
aggregation operator only returns an aggregate for its window.

But, the problem of late data still remains. How can we know whether we have
seen all the data belonging to a window? For this, the idea is that we do not
know. Instead, we can _trigger_ an output and later correct the output when a
late input record arrives. Triggering helps freshness. Refinements help
correctness. 

There are several triggering strategies such as trigger an output of the window
that is one minute older than the current processing time, etc. There are three
correction strategies that the paper describes:
* discarding: just throw away new data 
* accumulating: write a new record with the new output
* accumulating and retracting: write a new record with the new output and 
  retract the old record.

Discarding makes sense when you don't care about dropping late records.
Accumulating makes sense if the output was being written to a database and is
now just overwritten. Accumulating and retracting makes sense if there were
further downstream operators.

> If we are performing aggregations downstream from session creation that 
> depend on properties of the sessions themselves, for example detecting 
> unpopular ads (such as those which are viewed for less than five seconds in a 
> majority of sessions), initial results may be invalidated as inputs evolve 
> over time, e.g. as a significant number of offline mobile viewers come back 
> online and upload session data. Retractions provide a way for us to adapt to 
> these types of changes in complex pipelines with multiple serial grouping 
> stages.

For correctness, accumulating and retracting is the most general. Note that
retractions and modifications are only required to handle late data. If we are
doing "arrival-time" windows then there is no late data:

> Another, more robust way of providing processing-time windowing semantics 
> is to simply assign arrival time as event times at data ingress, then use 
> event time windowing. A nice side effect of using arrival time event times is 
> that the system has perfect knowledge of the event times in flight, and thus 
> can provide perfect (i.e. non-heuristic) watermarks, with no late data. This 
> is an effective and cost-efficient way of processing unbounded data for use 
> cases where true event times are not necessary or available.

**Structured streaming in Spark** [VLDB'18] again talks about watermarks but
their purpose is for cleaning up. The issue is that let us say that we get very
late data, like one year old, then we will have to remember the output of one
year old computation and then retract it. .

> In practice, however, it is useful for the processing system to have
> some loose bounds on how late data can arrive, for two reasons:
> 
> 1. Allowing arbitrarily late data might require storing arbitrarily
> large state. For example, if we count data by 1-minute event
> time window, the system needs to remember a count for every
> 1-minute window since the application began, because a late
> record might still arrive for any particular minute. This can
> quickly lead to large amounts of state, especially if combined
> with another grouping key. The same issue happens with joins.
> 2. Some sinks do not support data retraction, making it useful to
> be able to write the results for a given event time after a timeout.
> For example, custom downstream applications want to start
> working with a “final" result and might not support retractions.
> Append-mode sinks also do not support retractions.

The point 2 above is more like low watermarks: they unblock computation. Similar
to the three correction strategies discussed in the dataflow model, this paper
talks about three output strategies which directly correspond with the dataflow
model:

> The sink’s output mode specifies how the result table is written
> to the output system. The engine supports three distinct modes:
> * Complete: The engine writes the whole result table at once,
> e.g., replacing a whole file in HDFS with a new version. This
> is of course inefficient when the result is large.
> * Append: The engine can only add records to the sink. For
> example, a map-only job on a set of input files results in
> monotonically increasing output.
> * Update: The engine updates the sink in place based on a key
> for each record, updating only keys whose values changed.

But, Spark only applies these three strategies during outputting. To simplify
the API, Spark does not allow discarding updates etc in the middle of the
computation. Therefore, Sparks' structured streaming _requires_ all the
operators in a streaming computation to be incremental.

> The next step of the query planning process is incrementalizing the static 
> query provided by the user to efficiently update results in response to new
> data. In general, Structured Streaming’s incrementalizer aims to ensure that
> the query’s result can be updated in time proportional to the amount of new
> data received before each trigger or to the amount of new rows that have to be
> produced, without a dependance on the total amount of data received so far.